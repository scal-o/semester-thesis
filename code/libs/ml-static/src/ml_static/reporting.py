from __future__ import annotations

from typing import TYPE_CHECKING

import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from scipy.stats import norm
from sklearn.metrics import mean_squared_error, r2_score
from torch_geometric.loader import DataLoader

if TYPE_CHECKING:
    from ml_static.data import STADataset


## ==============================
## ===== compute functions ======
## ==============================
def compute_predictions(
    model: torch.nn.Module,
    dataset: STADataset,
) -> pd.DataFrame:
    """
    Generates a DataFrame with predictions and true values for a given dataset.

    Args:
        model: The trained GNN model.
        dataset: The dataset for the data split (train/val/test).

    Returns:
        A pandas DataFrame with detailed prediction results in original scale.
    """
    # create dataloader for efficient batch processing
    dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)

    all_predictions = []
    all_true_values = []

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    model.eval()
    with torch.no_grad():
        for data in dataloader:
            data = data.to(device)
            predictions = model(data).squeeze()
            true_values = data.y

            all_predictions.append(predictions.cpu())
            all_true_values.append(true_values.cpu())

    # flatten the lists of arrays into single tensors
    all_predictions = torch.cat(all_predictions)
    all_true_values = torch.cat(all_true_values)

    # apply inverse transform if available
    transform = getattr(dataset, "transform", None)
    if transform and hasattr(transform, "inverse_transform"):
        all_predictions = transform.inverse_transform(all_predictions)
        all_true_values = transform.inverse_transform(all_true_values)

    # convert to numpy for DataFrame
    all_predictions = all_predictions.numpy()
    all_true_values = all_true_values.numpy()

    # create a DataFrame
    df = pd.DataFrame({"true_value": all_true_values, "prediction": all_predictions})

    return df


def compute_errors(df: pd.DataFrame) -> pd.DataFrame:
    """
    Computes error metrics and adds them as columns to the DataFrame.

    Args:
        df: DataFrame generated by compute_predictions.

    Returns:
        The original DataFrame with additional columns for error metrics.
    """

    # calculate error columns
    df["error"] = df["prediction"] - df["true_value"]
    df["absolute_error"] = df["error"].abs()
    df["percentage_error"] = (df["error"] / df["true_value"]) * 100
    df["absolute_percentage_error"] = df["percentage_error"].abs()

    # handle division by zero
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    return df


def compute_statistics(pred_df: pd.DataFrame) -> pd.DataFrame:
    """
    Computes aggregate statistics from a prediction DataFrame.

    Args:
        pred_df: DataFrame generated by compute_predictions.

    Returns:
        A one-row pandas DataFrame containing the calculated statistics.
    """
    # calculate metrics
    mae = pred_df["absolute_error"].mean()
    rmse = np.sqrt(mean_squared_error(pred_df["true_value"], pred_df["prediction"]))
    r2 = r2_score(pred_df["true_value"], pred_df["prediction"])
    mape = pred_df["absolute_percentage_error"].mean()
    mdape = pred_df["absolute_percentage_error"].median()
    error_std = pred_df["error"].std()

    # create a dict -> DataFrame
    stats = {
        "MAE": mae,
        "RMSE": rmse,
        "R2": r2,
        "MAPE (%)": mape,
        "MDAPE (%)": mdape,
        "Error Std Dev": error_std,
    }

    return pd.DataFrame([stats])


# def create_report_df(config_path, model_weights_path):
#     config_path = Path(config_path)
#     model_weights_path = Path(model_weights_path)
#     config = Config(config_path)
#     dataset = STADataset(config.dataset_path)
#     data = dataset[0]
#     model = GNN(data, config.hidden_channels, config.output_channels)
#     model.load_state_dict(torch.load(model_weights_path))
#     prediction = model(data)
#     df = pd.DataFrame()
#     df["pred"] = prediction.cpu().detach().numpy()
#     df["true"] = data["real"].edge_labels.cpu().detach().numpy()
#     df["diff"] = abs(df.pred - df.true)
#     df["pdiff"] = df["diff"] / df.true * 100
#     return df


## ===============================
## ===== plotting functions ======
## ===============================
def plot_performance_diagnostics(pred_df: pd.DataFrame, dataset_name: str):
    """
    Generates and returns a 1x3 grid of diagnostic plots for model performance:
    - Error Distribution Histogram
    - Actual vs. Predicted Scatter Plot
    - Residuals vs. Predicted Scatter Plot

    Args:
        pred_df: DataFrame generated by generate_prediction_df.
        dataset_name: The name of the dataset split (e.g., "Test").

    Returns:
        The matplotlib Figure object containing the plots.
    """
    fig, axes = plt.subplots(1, 3, figsize=(22, 6))
    fig.suptitle(f"Performance Diagnostics for {dataset_name} Set", fontsize=16)

    # 1. Error Distribution Histogram
    error_data = pred_df["percentage_error"].dropna()
    mu, std = norm.fit(error_data)
    axes[0].hist(error_data, bins=50, density=True, alpha=0.6, color="g")
    xmin, xmax = axes[0].get_xlim()
    x = np.linspace(xmin, xmax, 100)
    p = norm.pdf(x, mu, std)
    axes[0].plot(x, p, "k", linewidth=2)
    axes[0].set_title("Error Distribution (%)")
    axes[0].set_xlabel("Percentage Error")
    axes[0].set_ylabel("Density")
    axes[0].text(
        0.95,
        0.95,
        f"Mean: {mu:.2f}\nStd: {std:.2f}",
        transform=axes[0].transAxes,
        fontsize=10,
        verticalalignment="top",
        horizontalalignment="right",
        bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.5),
    )

    # 2. Actual vs. Predicted
    r2 = r2_score(pred_df["true_value"], pred_df["prediction"])
    axes[1].scatter(pred_df["true_value"], pred_df["prediction"], alpha=0.5)
    lims = [
        np.min([axes[1].get_xlim(), axes[1].get_ylim()]),  # min of both axes
        np.max([axes[1].get_xlim(), axes[1].get_ylim()]),  # max of both axes
    ]
    axes[1].plot(lims, lims, "r-", alpha=0.75, zorder=0)
    axes[1].set_xlabel("Actual Values")
    axes[1].set_ylabel("Predicted Values")
    axes[1].set_title("Actual vs. Predicted")
    axes[1].text(
        0.05,
        0.95,
        f"$R^2 = {r2:.3f}$",
        transform=axes[1].transAxes,
        fontsize=12,
        verticalalignment="top",
        bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.5),
    )
    axes[1].set_aspect("equal", adjustable="box")

    # 3. Residuals vs. Predicted
    axes[2].scatter(pred_df["prediction"], pred_df["error"], alpha=0.5)
    axes[2].axhline(y=0, color="r", linestyle="--")
    axes[2].set_xlabel("Predicted Values")
    axes[2].set_ylabel("Error (Residuals)")
    axes[2].set_title("Residuals vs. Predicted Values")

    plt.tight_layout(rect=(0, 0.03, 1, 0.95))

    # return fig (so it can be saved externally)
    return fig


def plot_predictions(
    model: torch.nn.Module,
    dataset: STADataset,
    dataset_name: str,
    scenario_index: int,
):
    """
    Plots the model's predictions for a given scenario, with separate plots for each direction
    of bi-directional links.

    Args:
        model: The trained GNN model.
        dataset: The dataset containing the scenario.
        scenario_index: The index of the scenario to plot.
    """

    # get the predictions and true data
    data = dataset[[scenario_index]]
    data = compute_predictions(model, data)
    data = compute_errors(data)

    # get the scenario name
    scenario_name = dataset.scenario_names[scenario_index]

    # load the network geometry
    # access link_data_dir from the underlying dataset
    link_data_dir = getattr(dataset, "link_data_dir", None)
    if link_data_dir is None:
        raise AttributeError("Dataset must have 'link_data_dir' attribute for plot_predictions")
    gdf = gpd.read_parquet(link_data_dir / scenario_name / f"{scenario_name}.parquet")

    # add predictions and true values to the GeoDataFrame
    gdf["predicted_flow"] = data["prediction"]
    gdf["actual_flow"] = data["true_value"]
    gdf["error"] = data["absolute_percentage_error"]

    # create a normalized direction identifier based on node ordering
    # direction 0: a_node < b_node, direction 1: a_node > b_node
    gdf["link_direction"] = (gdf["a_node"] > gdf["b_node"]).astype(int)

    # separate into two direction groups
    gdf_dir0 = gdf[gdf["link_direction"] == 0].copy()
    gdf_dir1 = gdf[gdf["link_direction"] == 1].copy()

    # calculate consistent color scales across both directions
    vmin_flow = min(gdf["actual_flow"].min(), gdf["predicted_flow"].min())
    vmax_flow = max(gdf["actual_flow"].max(), gdf["predicted_flow"].max())
    vmin_error = 0
    vmax_error = gdf["error"].max()

    # create a single figure with 2 rows and 3 columns
    fig, axes = plt.subplots(2, 3, figsize=(20, 13), sharex=True, sharey=True)

    # plot both directions
    for row_idx, (direction, gdf_subset) in enumerate([(0, gdf_dir0), (1, gdf_dir1)]):
        if len(gdf_subset) == 0:
            continue

        # plot actual flow
        gdf_subset.plot(
            column="actual_flow", ax=axes[row_idx, 0], legend=True, vmin=vmin_flow, vmax=vmax_flow
        )
        axes[row_idx, 0].set_title(f"Actual Flow - Direction {direction}")

        # plot predicted flow
        gdf_subset.plot(
            column="predicted_flow",
            ax=axes[row_idx, 1],
            legend=True,
            vmin=vmin_flow,
            vmax=vmax_flow,
        )
        axes[row_idx, 1].set_title(f"Predicted Flow - Direction {direction}")

        # plot error
        gdf_subset.plot(
            column="error",
            ax=axes[row_idx, 2],
            legend=True,
            cmap="jet",
            vmin=vmin_error,
            vmax=vmax_error,
        )
        axes[row_idx, 2].set_title(f"Prediction Error (%) - Direction {direction}")

        # add error stats textbox
        min_error = gdf_subset["error"].min()
        max_error = gdf_subset["error"].max()
        avg_error = gdf_subset["error"].mean()
        textstr = f"Min: {min_error:.2f}%\nMax: {max_error:.2f}%\nAvg: {avg_error:.2f}%"
        props = dict(boxstyle="round", facecolor="wheat", alpha=0.5)
        axes[row_idx, 2].text(
            0.5,
            0.05,
            textstr,
            transform=axes[row_idx, 2].transAxes,
            fontsize=10,
            verticalalignment="bottom",
            horizontalalignment="center",
            bbox=props,
        )

    fig.suptitle(f"Dataset: {dataset_name}\nScenario: {scenario_name}\n", fontsize=16)
    plt.tight_layout()
    plt.show()
