from typing import Callable

import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from scipy.stats import norm
from sklearn.metrics import mean_squared_error, r2_score
from torch_geometric.loader import DataLoader

from ml_static.data import STADataset


## ==============================
## ===== compute functions ======
## ==============================
def compute_predictions(
    model: torch.nn.Module,
    dataloader: DataLoader,
    target_getter: Callable,
) -> pd.DataFrame:
    """
    Generates a DataFrame with predictions and true values for a given dataset.

    Args:
        model: The trained GNN model.
        dataloader: The dataloader for the dataset split (train/val/test).
        device: The device to run the model on.
        target_getter: A function to extract the target values from the data object.

    Returns:
        A pandas DataFrame with detailed prediction results.
    """
    all_predictions = []
    all_true_values = []

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    model.eval()
    with torch.no_grad():
        for data in dataloader:
            data = data.to(device)
            predictions = model(data).squeeze()
            true_values = target_getter(data)

            all_predictions.append(predictions.cpu().numpy())
            all_true_values.append(true_values.cpu().numpy())

    # flatten the lists of arrays into single numpy arrays
    all_predictions = np.concatenate(all_predictions)
    all_true_values = np.concatenate(all_true_values)

    # create a DataFrame
    df = pd.DataFrame({"true_value": all_true_values, "prediction": all_predictions})

    # calculate error columns
    df["error"] = df["prediction"] - df["true_value"]
    df["absolute_error"] = df["error"].abs()
    df["percentage_error"] = (df["error"] / df["true_value"]) * 100
    df["absolute_percentage_error"] = df["percentage_error"].abs()

    # handle division by zero
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    return df


def compute_statistics(pred_df: pd.DataFrame) -> pd.DataFrame:
    """
    Computes aggregate statistics from a prediction DataFrame.

    Args:
        pred_df: DataFrame generated by compute_predictions.

    Returns:
        A one-row pandas DataFrame containing the calculated statistics.
    """
    # calculate metrics
    mae = pred_df["absolute_error"].mean()
    rmse = np.sqrt(mean_squared_error(pred_df["true_value"], pred_df["prediction"]))
    r2 = r2_score(pred_df["true_value"], pred_df["prediction"])
    mape = pred_df["absolute_percentage_error"].mean()
    mdape = pred_df["absolute_percentage_error"].median()
    error_std = pred_df["error"].std()

    # create a dict -> DataFrame
    stats = {
        "MAE": mae,
        "RMSE": rmse,
        "R2": r2,
        "MAPE (%)": mape,
        "MDAPE (%)": mdape,
        "Error Std Dev": error_std,
    }

    return pd.DataFrame([stats])


# def create_report_df(config_path, model_weights_path):
#     config_path = Path(config_path)
#     model_weights_path = Path(model_weights_path)
#     config = Config(config_path)
#     dataset = STADataset(config.dataset_path)
#     data = dataset[0]
#     model = GNN(data, config.hidden_channels, config.output_channels)
#     model.load_state_dict(torch.load(model_weights_path))
#     prediction = model(data)
#     df = pd.DataFrame()
#     df["pred"] = prediction.cpu().detach().numpy()
#     df["true"] = data["real"].edge_labels.cpu().detach().numpy()
#     df["diff"] = abs(df.pred - df.true)
#     df["pdiff"] = df["diff"] / df.true * 100
#     return df


## ===============================
## ===== plotting functions ======
## ===============================
def plot_performance_diagnostics(pred_df: pd.DataFrame, dataset_name: str):
    """
    Generates and returns a 1x3 grid of diagnostic plots for model performance:
    - Error Distribution Histogram
    - Actual vs. Predicted Scatter Plot
    - Residuals vs. Predicted Scatter Plot

    Args:
        pred_df: DataFrame generated by generate_prediction_df.
        dataset_name: The name of the dataset split (e.g., "Test").

    Returns:
        The matplotlib Figure object containing the plots.
    """
    fig, axes = plt.subplots(1, 3, figsize=(22, 6))
    fig.suptitle(f"Performance Diagnostics for {dataset_name} Set", fontsize=16)

    # 1. Error Distribution Histogram
    error_data = pred_df["percentage_error"].dropna()
    mu, std = norm.fit(error_data)
    axes[0].hist(error_data, bins=50, density=True, alpha=0.6, color="g")
    xmin, xmax = axes[0].get_xlim()
    x = np.linspace(xmin, xmax, 100)
    p = norm.pdf(x, mu, std)
    axes[0].plot(x, p, "k", linewidth=2)
    axes[0].set_title("Error Distribution (%)")
    axes[0].set_xlabel("Percentage Error")
    axes[0].set_ylabel("Density")
    axes[0].text(
        0.95,
        0.95,
        f"Mean: {mu:.2f}\nStd: {std:.2f}",
        transform=axes[0].transAxes,
        fontsize=10,
        verticalalignment="top",
        horizontalalignment="right",
        bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.5),
    )

    # 2. Actual vs. Predicted
    r2 = r2_score(pred_df["true_value"], pred_df["prediction"])
    axes[1].scatter(pred_df["true_value"], pred_df["prediction"], alpha=0.5)
    lims = [
        np.min([axes[1].get_xlim(), axes[1].get_ylim()]),  # min of both axes
        np.max([axes[1].get_xlim(), axes[1].get_ylim()]),  # max of both axes
    ]
    axes[1].plot(lims, lims, "r-", alpha=0.75, zorder=0)
    axes[1].set_xlabel("Actual Values")
    axes[1].set_ylabel("Predicted Values")
    axes[1].set_title("Actual vs. Predicted")
    axes[1].text(
        0.05,
        0.95,
        f"$R^2 = {r2:.3f}$",
        transform=axes[1].transAxes,
        fontsize=12,
        verticalalignment="top",
        bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.5),
    )
    axes[1].set_aspect("equal", adjustable="box")

    # 3. Residuals vs. Predicted
    axes[2].scatter(pred_df["prediction"], pred_df["error"], alpha=0.5)
    axes[2].axhline(y=0, color="r", linestyle="--")
    axes[2].set_xlabel("Predicted Values")
    axes[2].set_ylabel("Error (Residuals)")
    axes[2].set_title("Residuals vs. Predicted Values")

    plt.tight_layout(rect=(0, 0.03, 1, 0.95))

    # return fig (so it can be saved externally)
    return fig


def plot_predictions(
    model: torch.nn.Module,
    dataset: STADataset,
    dataset_name: str,
    scenario_index: int,
):
    """
    Plots the model's predictions for a given scenario.

    Args:
        model: The trained GNN model.
        dataset: The dataset containing the scenario.
        scenario_index: The index of the scenario to plot.
        device: The device to run the model on.
    """

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # get the scenario data
    data = dataset[scenario_index].to(device)

    # get the model's predictions
    model.eval()
    with torch.no_grad():
        predictions = model(data).squeeze()

    # get the true values
    true_values = data["nodes", "real", "nodes"].edge_labels

    # get the scenario name
    scenario_name = dataset.scenario_names[scenario_index]

    # load the network geometry
    gdf = gpd.read_parquet(dataset.link_data_dir / scenario_name / f"{scenario_name}.parquet")

    # add predictions and true values to the GeoDataFrame
    gdf["predicted_flow"] = predictions.cpu().numpy()
    gdf["actual_flow"] = true_values.cpu().numpy()
    gdf["error"] = ((gdf["predicted_flow"] - gdf["actual_flow"]) / gdf["actual_flow"]) * 100
    gdf["error"] = gdf["error"].fillna(0)  # handle division by zero

    # create the plots
    fig, axes = plt.subplots(1, 3, figsize=(20, 10), sharex=True, sharey=True)

    # get min and max for color map
    vmin = min(gdf["actual_flow"].min(), gdf["predicted_flow"].min())
    vmax = max(gdf["actual_flow"].max(), gdf["predicted_flow"].max())

    # plot actual flow
    gdf.plot(column="actual_flow", ax=axes[0], legend=True, vmin=vmin, vmax=vmax)
    axes[0].set_title("Actual Flow")

    # plot predicted flow
    gdf.plot(column="predicted_flow", ax=axes[1], legend=True, vmin=vmin, vmax=vmax)
    axes[1].set_title("Predicted Flow")

    # get min and max for error plot
    min_error = gdf["error"].min()
    max_error = gdf["error"].max()

    # plot error, capping the color map at +/- 100%
    gdf.plot(
        column="error",
        ax=axes[2],
        legend=True,
        cmap="coolwarm",
        vmin=-100,
        vmax=100,
    )
    axes[2].set_title("Prediction Error (%)")

    # add a textbox with the actual min/max error
    textstr = f"Min Error: {min_error:.2f}%\nMax Error: {max_error:.2f}%"
    props = dict(boxstyle="round", facecolor="wheat", alpha=0.5)
    axes[2].text(
        0.5,
        0.05,
        textstr,
        transform=axes[2].transAxes,
        fontsize=10,
        verticalalignment="bottom",
        horizontalalignment="center",
        bbox=props,
    )

    fig.suptitle(f"Dataset: {dataset_name}\nScenario: {scenario_name}", fontsize=16)
    plt.tight_layout()
    plt.show()
